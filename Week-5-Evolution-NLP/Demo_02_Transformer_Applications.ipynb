{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b2eaba0-e4d5-430f-8cdc-5db4e71eed2e",
   "metadata": {
    "id": "0b2eaba0-e4d5-430f-8cdc-5db4e71eed2e"
   },
   "source": [
    "# __Demo: Transformer Applications__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cf3c8-157a-4acf-bf13-7cf78d407bb5",
   "metadata": {
    "id": "267cf3c8-157a-4acf-bf13-7cf78d407bb5"
   },
   "source": [
    "# __Context:__\n",
    "\n",
    "The text below includes two sections, text1 and text2. `text1` presents a customer's negative review of the iPhone, while `text2` provides a positive review. This demo aims to demonstrate the Transformer models' ability to manage two distinct tasks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f21d21-4c0f-44bd-a205-499e1b05a953",
   "metadata": {
    "id": "c9f21d21-4c0f-44bd-a205-499e1b05a953"
   },
   "outputs": [],
   "source": [
    "text1 = '''Extremely disappointed with my recent iPhone purchase from Apple. The device constantly lags, and the battery life is abysmal,\n",
    "barely lasting through the day. Despite the hefty price tag, the performance is far from satisfactory. Customer support has been unhelpful,\n",
    "providing no viable solutions to address these persistant issues. This experience has left me regretting my decision to choose Apple,\n",
    "and I expected much better from a company known for its premium products.'''\n",
    "\n",
    "text2 = '''I recently purchased an iPhone from Apple, and it has been an absolute delight! The device runs smoothly, and the battery life is impressive, easily lasting throughout the day.\n",
    "The price, though high, is justified by the excellent performance and top-notch customer support. I am thoroughly satisfied with my decision to choose Apple, and it reaffirms their reputation\n",
    "for delivering premium products. Highly recommended for anyone seeking a reliable and high-performance smartphone'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3503b3-cc2b-4889-86d6-009f3f6ee49f",
   "metadata": {
    "id": "bf3503b3-cc2b-4889-86d6-009f3f6ee49f"
   },
   "source": [
    "# __Task 1: Text Classification__\n",
    "- Analyze customer reviews of an iPhone purchase to classify the sentiment as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad9066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0644d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\anaconda3\\envs\\agentvenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e108ccbb61e44a79b1ad08a5bd86e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063bf353dad9479daae708499944c8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b60ad4082d4693b1572bc1dc16db65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 Sentiment: [{'label': 'NEGATIVE', 'score': 0.9997414946556091}]\n",
      "Text 2 Sentiment: [{'label': 'POSITIVE', 'score': 0.9998181462287903}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\") \n",
    "result1 = classifier(text1)\n",
    "result2 = classifier(text2)\n",
    "print(\"Text 1 Sentiment:\", result1)\n",
    "print(\"Text 2 Sentiment:\", result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0a3781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why classifier = pipeline(\"text-classification\") is for senement analysis?\n",
    "# The line classifier = pipeline(\"text-classification\") initializes a text classification pipeline using the Hugging Face Transformers library.\n",
    "# Text classification is a common natural language processing (NLP) task that involves categorizing text into predefined classes or labels.\n",
    "# Sentiment analysis is a specific type of text classification where the goal is to determine the sentiment expressed in a piece of text, typically categorizing it as positive, negative, or neutral.\n",
    "# By using pipeline(\"text-classification\"), the code sets up a model that can analyze the sentiment of the provided text inputs (text1 and text2) and classify them accordingly.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da5f218f-96ba-4ca4-8f39-fa237ec0b050",
   "metadata": {
    "id": "da5f218f-96ba-4ca4-8f39-fa237ec0b050",
    "outputId": "bb460f09-01d9-4ee4-9931-e75ac4e1e411"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.999741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.999741"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "#text classification\n",
    "classifier = pipeline(\"text-classification\")\n",
    "import pandas as pd\n",
    "\n",
    "# Analyze for text1\n",
    "outputs1 = classifier(text1)\n",
    "pd.DataFrame(outputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef201ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15835ef1-8663-4e43-8bcd-69498cd2f3b0",
   "metadata": {
    "id": "15835ef1-8663-4e43-8bcd-69498cd2f3b0"
   },
   "source": [
    "__Note:__ The output displays a label with its corresponding probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41c3d939-b400-4023-8657-49b6dd974662",
   "metadata": {
    "id": "41c3d939-b400-4023-8657-49b6dd974662",
    "outputId": "deec9711-7d12-4efc-db25-05de5a76a30d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.999818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  POSITIVE  0.999818"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ananlyze for text2:\n",
    "outputs2 = classifier(text2)\n",
    "pd.DataFrame(outputs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197ef88-c42e-4f51-9795-6d71612cdf6e",
   "metadata": {
    "id": "7197ef88-c42e-4f51-9795-6d71612cdf6e"
   },
   "source": [
    "# __Task 2: Text Generation__\n",
    "\n",
    "- Generate a customer service response to one of the reviews. This is achieved by using a text generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f633d96-01d8-4ef1-9b2d-b25726da9615",
   "metadata": {
    "id": "6f633d96-01d8-4ef1-9b2d-b25726da9615",
    "outputId": "48c800b5-b2c2-40df-8cf9-a39f29b9b77d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176dbea2d9e449aea7e7ac5d93dbf032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\agentvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021d971a90ed4cf3911ba45daa633f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a661d3d821488d9e3b5da60a0ffac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e06f02f46c24c68a981615ea8cec260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b8d1362c7741f18060003ac94b013d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac47142f5f14446c81195626cda1c840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db53e329b1b4e49a319c6f12f6b1bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extremely disappointed with my recent iPhone purchase from Apple. The device constantly lags, and the battery life is abysmal,\n",
      "barely lasting through the day. Despite the hefty price tag, the performance is far from satisfactory. Customer support has been unhelpful,\n",
      "providing no viable solutions to address these persistant issues. This experience has left me regretting my decision to choose Apple,\n",
      "and I expected much better from a company known for its premium products.\n",
      "\n",
      "Customer service response:\n",
      "Dear Patron, Thanks for writing in! I am sorry to hear your experience with us. We have had some frustrating experiences with customer service across the board, and as with any good customer service company, there are some common problems such as\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42) # Set the seed to get reproducible results\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Dear Patron, Thanks for writing in! I am sorry to hear your experience with us.\"\n",
    "prompt = text1 + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=150)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2a9f0-9756-4335-a84f-d0bf7eaf368b",
   "metadata": {
    "id": "5de2a9f0-9756-4335-a84f-d0bf7eaf368b"
   },
   "source": [
    "__Note:__ This demonstrates how Transformers can create context-aware responses, which can be a valuable tool in automated customer service or similar applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2d20f-9660-46f6-838c-e943f1e8c5fb",
   "metadata": {
    "id": "a6c2d20f-9660-46f6-838c-e943f1e8c5fb"
   },
   "source": [
    "# __Conclusion__\n",
    "This demo aims to illustrate the practical utility of Transformer models in real-world scenarios, emphasizing their effectiveness in understanding and generating human-like text."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "agentvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
